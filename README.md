# ğŸ—“ï¸ Meeting Notes & Action Items Extractor

Automatically transform your **meeting audio recordings** into structured outputs:  
âœ… **Transcript**  
âœ… **Concise Summary**  
âœ… **Clear Action Items**  

Built with **open-source, offline-capable models**, this Streamlit app ensures your data stays privateâ€”no cloud APIs required (unless you choose to use them). Perfect for teams, freelancers, or students who want to streamline post-meeting workflows.

---

## ğŸ”‘ Key Features

- **Audio Upload**: Supports `.mp3`, `.wav`, `.m4a`, and other common formats.
- **Offline Transcription**: Uses **OpenAI Whisper** (CPU-friendly, runs locally).
- **Smart Summarization**: Leverages **Hugging Face Transformers** (e.g., BART) for digestible summaries.
- **Action Item Extraction**:
  - ğŸ”¹ **Rule-based**: Keyword-driven identification (e.g., "assign", "due", "need to").
  - ğŸ”¹ **LLM-powered (optional)**: Uses **LangChain + Ollama** with local models like `llama2` or `mistral` for deeper semantic understanding.
- **100% Offline Mode**: No internet or API keys needed if using local models only.
- **Clean UI**: Streamlit-powered interface for seamless upload and results viewing.

---

## ğŸ› ï¸ Tech Stack

| Component             | Technology                         |
|-----------------------|------------------------------------|
| **Frontend**          | Streamlit                          |
| **Transcription**     | OpenAI Whisper (local)             |
| **Summarization**     | Hugging Face `transformers`        |
| **Action Extraction** | Custom logic **or** LangChain + Ollama |
| **LLM Backend**       | Ollama (`llama2`, `mistral`, etc.) |
| **Environment**       | Python 3.10+, `.env` support       |

---

## ğŸ“‚ Project Structure

```
meeting-notes-extractor/
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ notes.py                # Streamlit UI
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ transcription.py        # Whisper audio â†’ text
â”‚   â”œâ”€â”€ summarizer.py           # Text â†’ summary
â”‚   â””â”€â”€ action_extractor.py     # Summary â†’ action items
â”œâ”€â”€ app.py                      # (Optional main entry)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables (optional)
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Prerequisites

- **Python 3.10+**
- **FFmpeg** (required by Whisper for audio processing)
- **Ollama** (optional, only if using LLM for action items):  
  ```bash
  ollama pull llama2  # or mistral, etc.
  ```

---

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/lekshmi-c/MeetingNoteExtractor.git
cd meeting-notes-extractor

# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate        # macOS/Linux
# or
.venv\Scripts\activate           # Windows

# Install core dependencies
pip install -r requirements.txt

# (Optional) Install LangChain + Ollama support
pip install langchain-community langchain-ollama

# Install Whisper (already in requirements.txt, but ensure it's CPU-compatible)
pip install openai-whisper
```

> ğŸ’¡ **Note**: Whisper runs on CPU by defaultâ€”no GPU required.

---

## â–¶ï¸ Running the App

```bash
streamlit run frontend/notes.py
```

Open your browser to:  
ğŸ‘‰ **http://localhost:8501**

---

## ğŸ§ª How to Use

1. **Upload** a meeting audio file (e.g., `team_sync.wav`).
2. The app will:
   - **Transcribe** the audio using Whisper,
   - **Summarize** the full transcript,
   - **Extract action items** using either:
     - Built-in keyword logic, **or**
     - A local LLM (if Ollama is configured).
3. View all three sections clearly in the UI.

---

## âš™ï¸ Configuration

### Optional: Enable Local LLM for Action Items

1. In `backend/action_extractor.py`, uncomment and configure:
   ```python
   from langchain_ollama import OllamaLLM
   llm = OllamaLLM(model="llama2")
   ```
2. Ensure Ollama is running:
   ```bash
   ollama run llama2
   ```

### Optional: Use OpenAI (Not Recommended for Privacy)

Create a `.env` file:
```env
OPENAI_API_KEY=sk-...
```
> Only needed if you replace local models with OpenAI services (not default).

---

## ğŸ“ Example Output

| Section        | Example |
|----------------|--------|
| **Transcript** | *"We need to finalize the Q3 roadmap by Friday. John will handle the client demo."* |
| **Summary**    | *The team discussed Q3 deliverables and assigned the client demo to John.* |
| **Action Items** | `- Finalize Q3 roadmap by Friday<br>- John: Prepare client demo` |

---

## ğŸ Known Notes

- **First run may be slow**: Models are downloaded and cached on initial use.
- **Large audio files**: Processing time scales with duration (Whisper is CPU-bound).
- **Action item quality**: LLM-based extraction yields richer results than keyword matching.

---

## ğŸš€ Future Enhancements

- ğŸ§ **Speaker diarization** (who said what)
- ğŸ’¾ **Export to Markdown, PDF, or task managers** (e.g., Todoist, Notion)
- ğŸ“… **Auto-schedule reminders** from action items
- ğŸ³ **Docker support** for one-click deployment

---

## ğŸ“„ License

This project is free to use, modify, and distribute. See the repository for full license details.

---

> ğŸ’¡ **Tip**: Great for stand-ups, client calls, lectures, or interviewsâ€”turn hours of audio into actionable insights in minutes, all without leaving your machine!
